Metadata-Version: 2.4
Name: context-compressor
Version: 0.1.0
Summary: Intelligent context compression for LLM prompts
Author-email: Context Compressor Team <team@context-compressor.com>
License: MIT
Project-URL: Homepage, https://github.com/context-compressor/context-compressor
Project-URL: Repository, https://github.com/context-compressor/context-compressor
Project-URL: Documentation, https://context-compressor.readthedocs.io
Project-URL: Issues, https://github.com/context-compressor/context-compressor/issues
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.104.0
Requires-Dist: uvicorn[standard]>=0.24.0
Requires-Dist: pydantic>=2.5.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: tiktoken>=0.5.0
Requires-Dist: sentence-transformers>=2.2.0
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.35.0
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.7.0; extra == "dev"
Requires-Dist: pre-commit>=3.5.0; extra == "dev"
Requires-Dist: psutil>=5.9.0; extra == "dev"
Requires-Dist: httpx>=0.25.0; extra == "dev"

# Context Compressor

Intelligent context compression for LLM prompts that reduces token usage by 50-70% while maintaining quality.

## Overview

Context Compressor is a middleware that takes a query and many retrieved passages and returns a compact, diverse, citation-preserving context within a token budget. It implements a sophisticated three-stage pipeline:

1. **Rank Fusion**: Z-score normalization of dense and BM25 scores
2. **MMR Diversity**: Maximal Marginal Relevance selection with budget constraints
3. **Sentence Trimming**: Anchor-aware sentence selection with optional cross-encoder reranking

## Features

- **Token Reduction**: 50-70% vs baseline context
- **Quality Preservation**: Delta(EM/P@5) >= -1 pt on held-out QA sets
- **Fast Processing**: p95 <= 40ms for N <= 200 candidates
- **Deterministic**: Fixed-seed reproducibility
- **Production Ready**: FastAPI endpoint + CLI interface
- **Model Integration**: Support for state-of-the-art embedding and reranking models

## Installation

```bash
# Install with pip
pip install context-compressor

# Or install from source
git clone https://github.com/context-compressor/context-compressor.git
cd context-compressor
pip install -e .
```

## Quick Start

### CLI Usage

```bash
# Basic compression with sample data
python -m context_compressor.cli --query "What were the main findings?" --budget 50

# With custom parameters
python -m context_compressor.cli \
    --query "Analyze the results" \
    --budget 100 \
    --lambda 0.8 \
    --section-cap 3 \
    --use-reranker \
    --verbose

# With custom candidates file
python -m context_compressor.cli \
    --query "Summarize the study" \
    --budget 75 \
    --candidates candidates.json \
    --output results.json
```

### Python API

```python
from context_compressor import ContextCompressor, CompressionRequest, Candidate

# Create sample candidates
candidates = [
    Candidate(
        id="c_001",
        doc_id="d_01",
        section="Results",
        page=14,
        text="The study found a 25% improvement in accuracy...",
        tokens=25,
        bm25=7.2,
        dense_sim=0.81,
        embedding=[0.1, 0.2, 0.3, 0.4, 0.5]
    ),
    # ... more candidates
]

# Create request
request = CompressionRequest(
    q="What were the main findings?",
    B=100,
    candidates=candidates,
    params={
        "fusion_weights": {"dense": 0.7, "bm25": 0.3},
        "lambda": 0.7,
        "section_cap": 2,
        "use_reranker": True,
        "embedding_model": "intfloat/e5-large-v2",
        "reranker_model": "BAAI/bge-reranker-large"
    }
)

# Compress context
compressor = ContextCompressor()
response = compressor.compress(request)

print(f"Compressed context: {response.context}")
print(f"Tokens used: {response.stats.used}/{response.stats.budget}")
```

### FastAPI Server

```bash
# Start the server
python integrated_qa_system.py

# Or with custom settings
python integrated_qa_system.py --host 0.0.0.0 --port 8000
```

The server provides endpoints for:
- `POST /qa`: Process questions with PDF documents
- `GET /health`: Health check
- `GET /metrics`: System metrics

## Advanced Features

### Cross-Document Compression (CDC)

For multi-document scenarios, Context Compressor supports CDC with automatic routing between single-document and cross-document modes:

```python
from context_compressor import ContextCompressor, CompressionParams

# Enable CDC with auto-router
params = CompressionParams(
    lambda_=0.7,
    doc_cap=5,
    topM=200,
    auto_router=True
)

request = CompressionRequest(
    q="What are the main findings?",
    B=500,
    candidates=candidates,
    params=params
)
```

### Multimodal Compression

Support for text, images, and tables with intelligent modality fusion:

```python
from context_compressor import EnhancedMultimodalCompressor, MultimodalCompressionRequest

compressor = EnhancedMultimodalCompressor()

request = MultimodalCompressionRequest(
    q="What does the data show?",
    B=400,
    candidates=multimodal_candidates,
    lambda_=0.5,
    enable_image_search=True,
    max_images=3,
    max_tables=5,
    image_weight=0.2
)

response = compressor.compress(request)
```

### LLM Integration

Seamless integration with OpenRouter and local LLMs:

```python
from openrouter_integration import OpenRouterIntegration
from llm_integration import OptimizedLLMIntegration

# OpenRouter integration
openrouter = OpenRouterIntegration(api_key="your-key", model_id="qwen/qwen3-8b:free")
response = openrouter.generate_response(compression_response, question)

# Local LLM integration
local_llm = OptimizedLLMIntegration("meta-llama/Llama-3.1-8B-Instruct", hf_token="your-token")
response = local_llm.generate_response(context, question)
```

## Configuration

### Environment Variables

```bash
# OpenRouter API
export OPENROUTER_API_KEY="your-openrouter-key"

# HuggingFace (for local models)
export HF_TOKEN="your-hf-token"

# Model paths
export CONTEXT_COMPRESSOR_MODEL_PATH="/path/to/models"
```

### Model Configuration

```python
# Custom embedding model
compressor = ContextCompressor(
    embedding_model="intfloat/e5-large-v2",
    reranker_model="BAAI/bge-reranker-large"
)

# Performance optimization
compressor = ContextCompressor(
    use_cache=True,
    batch_size=32,
    device="cuda"
)
```

## Performance

### Benchmarks

| Metric | Value | Target |
|--------|-------|--------|
| Token Reduction | 50-70% | >50% |
| Quality Delta | -1 pt | >=-1 pt |
| Processing Time | p95 <= 40ms | <=40ms |
| Memory Usage | <4GB | <4GB |

### Optimization

- **Caching**: Embedding and reranker model caching
- **Batching**: Efficient batch processing for large candidate sets
- **Memory Management**: GPU memory optimization and cleanup
- **Parallel Processing**: Concurrent operations where possible

## API Reference

### Core Classes

- `ContextCompressor`: Main compression engine
- `EnhancedMultimodalCompressor`: Multimodal compression
- `OpenRouterIntegration`: OpenRouter LLM integration
- `OptimizedLLMIntegration`: Local LLM integration

### Key Methods

- `compress()`: Main compression method
- `generate_response()`: LLM response generation
- `extract_from_pdf()`: PDF content extraction

## Development

### Setup

```bash
# Clone repository
git clone https://github.com/context-compressor/context-compressor.git
cd context-compressor

# Install dependencies
pip install -e ".[dev]"

# Run tests
pytest tests/

# Run linting
ruff check .
```

### Testing

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_compression.py
pytest tests/test_multimodal.py
pytest tests/test_integration.py

# Run with coverage
pytest --cov=context_compressor
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Citation

If you use Context Compressor in your research, please cite:

```bibtex
@article{context-compressor-2024,
  title={Context Compressor: Intelligent Context Compression for LLM Prompts},
  author={Your Name},
  journal={arXiv preprint},
  year={2024}
}
```
